{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNf++Zn2ZU1HSxxwZwCPPLP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Text processing"],"metadata":{"id":"W7liv6haBNZb"}},{"cell_type":"markdown","source":["## Environment setup"],"metadata":{"id":"fm2NRDIAqEzI"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":995,"status":"ok","timestamp":1753256365515,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"},"user_tz":-120},"id":"IQl1rr6FSpKO","outputId":"3e0bd6fe-6da7-4fde-f073-cc00af50f4c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["WORKING_PATH = '/content/drive/MyDrive/KeepCoding/NLP/exercise'\n"],"metadata":{"id":"ysmWbEeS5h5o","executionInfo":{"status":"ok","timestamp":1753256365520,"user_tz":-120,"elapsed":3,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1753256365551,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"},"user_tz":-120},"id":"TF0i-O5oTIwc","outputId":"29a3aa21-713f-4c36-d537-cdba04fee8a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/KeepCoding/NLP/exercise\n"]}],"source":["%cd {WORKING_PATH}"]},{"cell_type":"code","source":["!pip install -r requirements.txt"],"metadata":{"id":"T4InpvkuquUm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","from wordcloud import WordCloud, STOPWORDS\n","import unicodedata\n","from num2words import num2words\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import nltk\n","from nltk import pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import wordnet, stopwords\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('averaged_perceptron_tagger_eng')\n","nltk.download('stopwords')"],"metadata":{"id":"NSKbPus5P6b0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1753256431683,"user_tz":-120,"elapsed":1276,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}},"outputId":"cbeb8d43-418c-436c-8c5e-14dafbea1a63"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# to load custom libraries\n","sys.path.append(WORKING_PATH)\n","\n","# load custom libraries"],"metadata":{"id":"hqibiYKAE_ta","executionInfo":{"status":"ok","timestamp":1753256377472,"user_tz":-120,"elapsed":31,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Read data"],"metadata":{"id":"wPNbkmCgTKHq"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pyct7ps9okI1","executionInfo":{"status":"ok","timestamp":1753256378487,"user_tz":-120,"elapsed":1012,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}},"outputId":"b94c95a8-9ddb-4305-8467-b84aff7c4a68"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(13272, 9)"]},"metadata":{},"execution_count":6}],"source":["# Read data\n","data = pd.read_json('reviews_Patio_Lawn_and_Garden_5.json', lines=True)\n","data.shape"]},{"cell_type":"markdown","source":["## Function for normalizing texts"],"metadata":{"id":"xzi0hkCsTDNS"}},{"cell_type":"markdown","source":["This pipeline performs text normalization and lemmatization in preparation for natural language processing tasks. It begins by converting the input text to lowercase and expanding common negative contractions (e.g., \"don't\" becomes \"do not\"). Punctuation is isolated by surrounding it with spaces to improve tokenization. The text is then tokenized into individual words, and all instances of common negation words like \"not\" and \"nor\" are unified under the token \"no\" to preserve semantic consistency in downstream models and any numeric tokens are converted to their corresponding word form (e.g., \"3\" becomes \"three\"). Part-of-speech (POS) tags are assigned to each token and these tags are mapped to WordNet-compatible POS categories. Finally, the tokens are lemmatized (reduced to their base forms) using POS context, while stopwords and punctuation are filtered out, except for the token \"no\", which is retained due to its significance in sentiment and negation detection. The resulting list of normalized and lemmatized tokens is then returned for further analysis or modeling."],"metadata":{"id":"27HaryW5Vsew"}},{"cell_type":"code","source":["# Normalize texts\n","PUNCTUATION = set(string.punctuation)\n","STOP_WORDS = set(stopwords.words('english'))\n","\n","def get_wordnet_pos(treebank_tag):\n","  \"\"\"\n","  Convert POS tags to WordNet format\n","  \"\"\"\n","  if treebank_tag.startswith('J'):\n","      return wordnet.ADJ\n","  elif treebank_tag.startswith('V'):\n","      return wordnet.VERB\n","  elif treebank_tag.startswith('N'):\n","      return wordnet.NOUN\n","  elif treebank_tag.startswith('R'):\n","      return wordnet.ADV\n","  else:\n","      return wordnet.NOUN  # Default to noun\n","\n","\n","def lemmatize_text(pos_tags):\n","  \"\"\"\n","  Lemmatize words from POS-tagged tokens while filtering stopwords and punctuation\n","  \"\"\"\n","  lemmatizer = WordNetLemmatizer()\n","  lemmatized = []\n","  for word, tag in pos_tags:\n","    if (word == 'no') or (word not in STOP_WORDS and word not in PUNCTUATION):\n","      pos = get_wordnet_pos(tag)\n","      lemma = lemmatizer.lemmatize(word, pos=pos)\n","      lemmatized.append(lemma)\n","  #return ' '.join(lemmatized)\n","  return lemmatized\n","\n","def review2words(text):\n","  \"\"\"\n","  Performs a complete text normalization pipeline including:\n","    1. Case normalization\n","    2. Contraction expansion (negations)\n","    3. Punctuation isolation\n","    4. Tokenization\n","    5. Number-to-word conversion\n","    6. POS-aware lemmatization\n","  \"\"\"\n","  # text to lowercase\n","  text = text.lower()\n","  # Transform negative contractions (don't --> do not)\n","  text = re.sub(r\"n't\", \" not\", text)\n","  # Add spaces before and after punctuation marks.\n","  pattern = re.compile(f\"[{re.escape(''.join(PUNCTUATION))}]\")\n","  text = pattern.sub(f' {\" \"} ', text)\n","  # Tokenize\n","  tokens = word_tokenize(text)\n","  # Normalize negative words (n't, not, nor)\n","  tokens = list(map(lambda x: 'no' if x in [\"n't\", 'not', 'nor'] else x, tokens))\n","  # Transform digit to number\n","  tokens = list(map(lambda x: num2words(x, ordinal=False) if x.isdigit() else x, tokens))\n","  # POS tag\n","  pos_tags = pos_tag(tokens)\n","  # Lemmatize with POS context\n","  lematized = lemmatize_text(pos_tags)\n","  return lematized\n","\n","def overall2label(overall):\n","  \"\"\"\n","  Converts a numerical 'overall' rating into a binary label:\n","  - 0 for ratings below 4 (negative)\n","  - 1 for ratings 4 or above (positive)\n","  \"\"\"\n","  label = None\n","  if overall < 4:\n","    label = 0\n","  else:\n","    label = 1\n","  return label"],"metadata":{"id":"3ANCagsnto_5","executionInfo":{"status":"ok","timestamp":1753256470729,"user_tz":-120,"elapsed":25,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Check processing\n","print(f'Before:\\n{data.loc[201,\"reviewText\"]}')\n","print(f'After:\\n{review2words(data.loc[201,\"reviewText\"])}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIU_FBd3DELD","executionInfo":{"status":"ok","timestamp":1753256478882,"user_tz":-120,"elapsed":3317,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}},"outputId":"05de13e8-aa33-4300-e07c-addab4e55141"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Before:\n","This chain fit great for my Poulan Pro electric saw. This is also an older company that has a great website to help identify exactly what model your specific saw requires. It is not directional, so you don't have to worry about any arrows facing the right way. I just used it and I forgot how powerful my saw was. Getting old ones sharpened can be questionable when new ones are this cheap.\n","After:\n","['chain', 'fit', 'great', 'poulan', 'pro', 'electric', 'saw', 'also', 'old', 'company', 'great', 'website', 'help', 'identify', 'exactly', 'model', 'specific', 'saw', 'require', 'no', 'directional', 'no', 'worry', 'arrow', 'face', 'right', 'way', 'use', 'forget', 'powerful', 'saw', 'get', 'old', 'one', 'sharpen', 'questionable', 'new', 'one', 'cheap']\n"]}]},{"cell_type":"markdown","source":["## Preprocess data"],"metadata":{"id":"hBQU2PjBUC8z"}},{"cell_type":"markdown","source":["In this step, the data is prepared to be directly analyzed by a model. First, the target variable is created from the 'overall' variable. Then, the dataset is split into train and test sets, and text normalization is applied to both. The result is saved for later use."],"metadata":{"id":"omkRdVsDUqOQ"}},{"cell_type":"code","source":["cache_dir = \"cache\"\n","os.makedirs(cache_dir, exist_ok=True)\n","\n","def preprocess_data(data_train, data_test, labels_train, labels_test,\n","                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n","    \"\"\"\n","    Preprocesses training and test data by:\n","    1. Converting reviews to tokenized words\n","    2. Caching/loading processed data for efficiency\n","    \"\"\"\n","\n","    cache_data = None\n","    if cache_file is not None:\n","        try:\n","            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n","                cache_data = pickle.load(f)\n","            print(\"Read preprocessed data from cache file:\", cache_file)\n","        except:\n","            pass\n","\n","    if cache_data is None:\n","        words_train = list(map(review2words, data_train))\n","        words_test = list(map(review2words, data_test))\n","\n","        if cache_file is not None:\n","            cache_data = dict(words_train=words_train, words_test=words_test,\n","                              labels_train=labels_train, labels_test=labels_test)\n","            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n","                pickle.dump(cache_data, f)\n","            print(\"Wrote preprocessed data to cache file:\", cache_file)\n","    else:\n","        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n","                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n","\n","    return words_train, words_test, labels_train, labels_test"],"metadata":{"id":"t53LuzrBPXMi","executionInfo":{"status":"ok","timestamp":1753256770615,"user_tz":-120,"elapsed":3,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Create 'label' feature\n","data['label'] = list(map(overall2label, data['overall']))\n","\n","# Split into train and test datasets\n","reviewText_train, reviewText_test, overall_train, overall_test = train_test_split(\n","    data['reviewText'], data['label'], train_size=0.75, test_size=0.25,\n","    random_state=42, shuffle=True, stratify=data['label'])\n","\n","words_train, words_test, labels_train, labels_test = preprocess_data(reviewText_train, reviewText_test, overall_train, overall_test, cache_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVJ6xYMBmDEs","executionInfo":{"status":"ok","timestamp":1753257500090,"user_tz":-120,"elapsed":127124,"user":{"displayName":"Miguel Angel Pardo","userId":"16900940354237523056"}},"outputId":"a6fdd97c-c71e-46c1-ca56-8381beebcfb7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote preprocessed data to cache file: preprocessed_data.pkl\n"]}]}]}